{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section III Text Classifiers in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Scikit-Learn**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(1) Background**\n",
    "- Open source machine learning library\n",
    "- More programmatic interface (compared to waka)\n",
    "##### **(2)Application**\n",
    "- **Using Sklearn's *Na&#239;ve Bayes Classifier***:\n",
    "\n",
    "    ```python\n",
    "    from sklearn import naive_bayes # Import Naive Bayes Model\n",
    "    clfrNB_mn = naive_bayes.MultinomialNB() # Use the multinomial Naive Bayes model\n",
    "    clfrNB_bnl = naive_bayes.BernoulliNB() # Use the Bernoulli Naive Bayes model\n",
    "    clfrNB.fit(train_data,train_labels) # Train the classifier using features and labels from the training data\n",
    "\n",
    "    # these two steps can be merged as:\n",
    "    clfrNB = naive_bayes.MultinomialNB.fit(train_data, train_labels)\n",
    "\n",
    "    predicted_labels = clfrNB.predict(test_data) # Use fitted model to predict the label for the test dataset\n",
    "\n",
    "    metrics.f1_score(test_labels, predict_labels, average = \"micro\") # Use F1 score to evaluate the performance of the model based on the predicted and actual test labels\n",
    "    ```\n",
    "- **Using Sklearn's *SVM Classifier***:\n",
    "    ```python\n",
    "    from sklearn import svm # Import SVM Model\n",
    "    clfrSVM = svm.SVC(kernel=\"linear\", C=0.1) # Use a linear kernel and 0.1 of regularization factor for the SVM classifier\n",
    "    clfrSVM.fit(train_data, train_labels) # Train the classifier using features and labels from the training data\n",
    "    predicted_labels = clfrNB.predict(test_data) # Use fitted model to predict the label for the test dataset\n",
    "    ```\n",
    "\n",
    "##### **(3) Model selection**\n",
    "- *Train-test split*\n",
    "    ```python\n",
    "    from sklearn import model_selection\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(train_data, train_labels, test_size = 0.333, random_state = 0) # Use a 2/3-1/3 split for training and test sets of the total dataset\n",
    "    ```\n",
    "    - **Way to perform**: Split the training data using a preset fraction into a group of data for <u>training the model</u> (training data) and a group of data for <u>tuning the model</u> (test data)\n",
    "    - **Advantage**: easy to perform\n",
    "    - **Disadvantage**: loses the fraction of test data for model tuning\n",
    "- *Cross validation*\n",
    "    ```python\n",
    "    predicted_labels = model_selection.cross_val_predict(clfrSVM, train_data, train_labels, cv=5) # perform a 5-fold cross validation on the training data\n",
    "    ```\n",
    "    - **Way to perform**: split the training data to a number of folds, hold one portion of the folds out for testing, use the remaining portions for training, and repeat it for the number of folds times repeatedly for every group of data\n",
    "    - **Advantage**: Improves the accuracy and reduces the variance of the evaluation process; all samples are included for training\n",
    "    - **Disadvantage**: More complex; more time is needed\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **NLTK**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(1) Availability**\n",
    "- Na&#239;ve Bayes Classifier\n",
    "- Decision Tree Classifier\n",
    "- Conditional Exponential Classifier\n",
    "- Maxent Classifier\n",
    "- Weka Classifier\n",
    "- Sklearn Classifer\n",
    "\n",
    "##### **(2) Application**\n",
    "- **Using NLTK's *Na&#239;ve Bayes Classifier***:\n",
    "\n",
    "    ```python\n",
    "    from nltk.classify import NaiveBayesClassifier # Import Naive Bayes Model\n",
    "    classifier = NaiveBayesClassifier.train(train_set) # Use the multinomial Naive Bayes model\n",
    "    classifier.classify(unlabeled_instance) # Use fitted model to predict the label for ONE test sample\n",
    "    classifier.classify_many(unlabeled_instances) # Use fitted model to predict the label for A SET OF test samples\n",
    "    \n",
    "    from nltk.classify import util # Import essential utilities\n",
    "    util.accuracy(classifier, test_set) # Get the accuracy of the performance of the sklearn classifier\n",
    "    classifier.labels() # Shows all the labels the classifier has trained on\n",
    "    classifier.show_most_informative_features() # Shows the most important features for the task\n",
    "    ```\n",
    "- **Using NLTK's *Sklearn Classifier* for *SVM Classifier***:\n",
    "    ```python\n",
    "    from nltk.classify import SklearnClassifier # Import Sklearn's classifier Model\n",
    "    from sklearn.svm import SVC\n",
    "    clfrNB = SklearnClassifier(SVC(), kernel=\"linear\").train(train_set) # Call SVM model by NLTK's SklearnClassifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***\\*Take Home Concepts 3***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ - <u>Scikit-learn</u> is the most commonly used ML toolkit in Python  \n",
    "$\\qquad$ - <u>NLTK</u> has its own Na&#239;ve Bayes implementation  \n",
    "$\\qquad$ - <u>NLTK</u> can also interface with Scikit-learn and Waka"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Case Study: *Sentiment Analysis***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The correct results should be:\n",
    "    - Percentage of \"positively rated\" instances: 0.74718\n",
    "    - Number of instances in train set X_train: 23052\n",
    "    - Number of features in vect: 19601\n",
    "    - AUC of logistic regression: 0.89743\n",
    "    - Number of features after TfldfVectorizer is fit: 5442\n",
    "    - AUC of logistic regression after TfldfVectorizer: 0.88995\n",
    "    - Number of features after n-grams: 29072\n",
    "    - AUC of logistic regression on n-grams: 0.91107"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(1) Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "      <th>Positively Rated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>1</td>\n",
       "      <td>I already had a phone with problems... I know ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>2</td>\n",
       "      <td>The charging port was loose. I got that solder...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>2</td>\n",
       "      <td>Phone looks good but wouldn't stay charged, ha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I originally was using the Samsung S2 Galaxy f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a great product it came after two days...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Product Name Brand Name   Price  \\\n",
       "0   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "5   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "6   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "7   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "8   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "11  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "    Rating                                            Reviews  Review Votes  \\\n",
       "0        5  I feel so LUCKY to have found this used (phone...           1.0   \n",
       "1        4  nice phone, nice up grade from my pantach revu...           0.0   \n",
       "2        5                                       Very pleased           0.0   \n",
       "3        4  It works good but it goes slow sometimes but i...           0.0   \n",
       "4        4  Great phone to replace my lost phone. The only...           0.0   \n",
       "5        1  I already had a phone with problems... I know ...           1.0   \n",
       "6        2  The charging port was loose. I got that solder...           0.0   \n",
       "7        2  Phone looks good but wouldn't stay charged, ha...           0.0   \n",
       "8        5  I originally was using the Samsung S2 Galaxy f...           0.0   \n",
       "11       5  This is a great product it came after two days...           0.0   \n",
       "\n",
       "    Positively Rated  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  1  \n",
       "5                  0  \n",
       "6                  0  \n",
       "7                  0  \n",
       "8                  1  \n",
       "11                 1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('assets/Amazon_Unlocked_Mobile.zip',compression='zip')\n",
    "df.dropna(inplace=True)\n",
    "df = df[df['Rating'] != 3] # assume these are neutral\n",
    "df['Positively Rated'] = np.where(df['Rating']>3, 1, 0) # Return '1' if rating>3, else return '0'\n",
    "\n",
    "# df = df.sample(frac=0.1, random_state=10)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7482686025879323"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(df['Positively Rated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'],df['Positively Rated'], random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train first entry:\n",
      "\n",
      " I feel so LUCKY to have found this used (phone to us & not used hard at all), phone on line from someone who upgraded and sold this one. My Son liked his old one that finally fell apart after 2.5+ years and didn't want an upgrade!! Thank you Seller, we really appreciate it & your honesty re: said used phone.I recommend this seller very highly & would but from them again!!\n",
      "\n",
      "\n",
      "X_train shape: (231207,)\n",
      "\n",
      "\n",
      "X_train first entry's label: 1\n"
     ]
    }
   ],
   "source": [
    "print('X_train first entry:\\n\\n', X_train[0])\n",
    "print('\\n\\nX_train shape:', X_train.shape)\n",
    "print('\\n\\nX_train first entry\\'s label:',y_train[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(2) CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '4less', 'adr6275', 'assignment', 'blazingly', 'cassettes',\n",
       "       'condishion', 'debi', 'dollarsshipping', 'esteem', 'flashy',\n",
       "       'gorila', 'human', 'irullu', 'like', 'microsaudered',\n",
       "       'nightmarish', 'p770', 'poori', 'quirky', 'responseive', 'send',\n",
       "       'sos', 'synch', 'trace', 'utiles', 'withstanding'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names_out()[::2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53216"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<231207x53216 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6117776 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Program Files\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9197254713325582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print(\"AUC: \", roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 smallest coefs:\n",
      "['worst' 'garbage' 'junk' 'unusable' 'false' 'worthless' 'useless'\n",
      " 'crashing' 'disappointing' 'awful']\n",
      "\n",
      "Top 10 largest coefs: \n",
      "['excelent' 'excelente' 'exelente' 'loving' 'loves' 'perfecto' 'excellent'\n",
      " 'awesome' 'complaints' 'buen']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Top 10 smallest coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Top 10 largest coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(3) TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect2 = TfidfVectorizer(min_df =5).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17951"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect2.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<231207x17951 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6056695 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized2 = vect2.transform(X_train)\n",
    "X_train_vectorized2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Program Files\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9265848398605042\n"
     ]
    }
   ],
   "source": [
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train_vectorized2, y_train)\n",
    "\n",
    "predictions2 = model2.predict(vect2.transform(X_test))\n",
    "\n",
    "print('AUC: ',roc_auc_score(y_test, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 smallest TFIDFs:\n",
      "['commenter' 'pthalo' 'warmness' 'storageso' 'aggregration' '1300'\n",
      " '625nits' 'a10' 'submarket' 'brawns']\n",
      "\n",
      "Top 10 largest TFIDFs: \n",
      "['defective' 'batteries' 'gooood' 'epic' 'luis' 'goood' 'basico'\n",
      " 'aceptable' 'problems' 'excellant']\n"
     ]
    }
   ],
   "source": [
    "feature_names2 = vect2.get_feature_names_out()\n",
    "\n",
    "sorted_tfidf_index2 = X_train_vectorized2.max(0).toarray()[0].argsort()\n",
    "\n",
    "print('Top 10 smallest TFIDFs:\\n{}\\n'.format(feature_names2[sorted_tfidf_index2[:10]]))\n",
    "print('Top 10 largest TFIDFs: \\n{}'.format(feature_names2[sorted_tfidf_index2[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 smallest coefs:\n",
      "['not' 'worst' 'useless' 'disappointed' 'terrible' 'return' 'waste' 'poor'\n",
      " 'horrible' 'doesn']\n",
      "\n",
      "Top 10 largest coefs: \n",
      "['love' 'great' 'excellent' 'perfect' 'amazing' 'awesome' 'perfectly'\n",
      " 'easy' 'best' 'loves']\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_index2 = model2.coef_[0].argsort()\n",
    "\n",
    "print('Top 10 smallest coefs:\\n{}\\n'.format(feature_names2[sorted_coef_index2[:10]]))\n",
    "print('Top 10 largest coefs: \\n{}'.format(feature_names2[sorted_coef_index2[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(vect.transform(['not an issue, phone is working','an issue, phone is not working'])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(4) n-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198917"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect3 = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "X_train_vectorized3 = vect3.transform(X_train)\n",
    "\n",
    "len(vect3.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Program Files\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9609307598776071\n"
     ]
    }
   ],
   "source": [
    "model3 = LogisticRegression()\n",
    "model3.fit(X_train_vectorized3, y_train)\n",
    "\n",
    "predictions3 = model3.predict(vect3.transform(X_test))\n",
    "\n",
    "print(\"AUC: \", roc_auc_score(y_test, predictions3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 smallest coefs:\n",
      "['no good' 'not happy' 'not worth' 'worst' 'junk' 'not satisfied'\n",
      " 'garbage' 'not good' 'terrible' 'defective']\n",
      "\n",
      "Top 10 largest coefs: \n",
      "['excelent' 'excelente' 'not bad' 'excellent' 'exelente' 'perfect'\n",
      " 'awesome' 'no problems' 'no issues' 'perfecto']\n"
     ]
    }
   ],
   "source": [
    "feature_names3 = vect3.get_feature_names_out()\n",
    "\n",
    "sorted_coef_index3 = model3.coef_[0].argsort()\n",
    "\n",
    "print('Top 10 smallest coefs:\\n{}\\n'.format(feature_names3[sorted_coef_index3[:10]]))\n",
    "print('Top 10 largest coefs: \\n{}'.format(feature_names3[sorted_coef_index3[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(model3.predict(vect3.transform(['not an issue, phone is working','an issue, phone is not working'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
