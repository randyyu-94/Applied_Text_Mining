{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section II Algorithms for Text Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Na&#239;ve Bayes Classifiers**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Case study: *Classifying text search queries***\n",
    "\n",
    "    - Classes of search queries to classify: <u>entertainment</u>, <u>computer science</u>, <u>zoology</u>\n",
    "    - The most common class: <u>entertainment</u>\n",
    "    - *Given* a query: *python*\n",
    "        - Refers to \"snake\" ~ <u>zoology</u>\n",
    "        - Refers to \"a programming language\" ~ <u>computer science</u>\n",
    "        - Refers to \"Monty Python\" ~ <u>entertainment</u>\n",
    "    - *Suppose* a query: *python download*\n",
    "        - most probably <u>computer science</u>\n",
    "\n",
    "- **Probabilistic model**\n",
    "    - **Principle**: have a basic probability; updates the probability by adding new information\n",
    "    - **Terms**\n",
    "        - *Prior probability*: the probability without any information\n",
    "            - e.g., Pr(y = \"Entertainment\") > Pr(y = \"CS\") > Pr(y = \"Zoology\")\n",
    "        - *Posterior probability*: the propability with infused new information\n",
    "            - e.g. Pr(y = \"Zoology\"|x = \"Python\") > Pr(y = \"CS\"|x = \"Python\") > Pr(y = \"Entertainment\"|x = \"Python\")\n",
    "\n",
    "- **Bayes' rule**\n",
    "    - **Equation**: \n",
    "    $$\\mathrm{Posterior\\ Probability} = \\frac{\\mathrm{Prior\\ Probability}\\ \\times\\ \\mathrm{Likelihood}}{\\mathrm{Evidence}}$$\n",
    "    $$\\mathrm{Pr}(y|X) = \\frac{\\mathrm{Pr}(y)\\times\\mathrm{Pr}(X|y)}{\\mathrm{Pr}(X)}$$\n",
    "\n",
    "- **Na&#239;ve Bayes Classification**\n",
    "    - **Assumption**: assume the probability of each feature is independent; $\\mathrm{Pr}(X) = 1$\n",
    "    - **Equation**: $y^* = \\mathrm{argmax}\\ \\mathrm{Pr}(y|X) = \\mathrm{argmax}\\ \\mathrm{Pr}(y)\\times\\mathrm{Pr}(X|y) = \\mathrm{argmax}\\ \\mathrm{Pr}(y)\\times\\prod\\limits_{i=1}^{n}\\mathrm{Pr}(x_i|y)$\n",
    "    - **Parameters**\n",
    "        - *prior probabilities*\n",
    "            - **Equation**: $\\mathrm{Pr}(y)$ for all $y$ in $Y$\n",
    "            - **How to obtain**: 1. count the occurrence of each $y$ ($n$) and all $Y$ ($N$) instances in the training data; 2. $\\mathrm{Pr}(y) = n/N$\n",
    "        - *likelihood*\n",
    "            - **Equation**: $\\mathrm{Pr}(x_i|y)$ for all features $x_i$ and labels $y$ in $Y$\n",
    "            - **How to obtain**: 1. count the occurrence of each $x_i$ ($k$) appears in instances labeled as class $y$ ($n$); 2. $\\mathrm{Pr}(X|y) = k/n$\n",
    "    - **Laplace/Additive smoothing**\n",
    "        - **Condition**: when $\\mathrm{Pr}(x_i|y)=0$, the *posterior probability* $\\mathrm{Pr}(y|X)$ becomes 0.\n",
    "        - **Action**: add a <u>dummy count</u> to every feature counts\n",
    "        - **Equation**: $\\mathrm{Pr}(x_i|y) = \\frac{k+1}{n+m}$, $m$ is the number of *features* for $x_i$\n",
    "\n",
    "- **Variants of Na&#239;ve Bayes Features**\n",
    "    - *Multinomial Na&#239;ve Bayes*\n",
    "        - Data follows a *multinomial* distribution\n",
    "        - Each feature value is a count\n",
    "        - <u>Frequency of words</u> is considered using weighting\n",
    "    - *Bernoulli Na&#239;ve Bayes*\n",
    "        - Data follows a *multivariate Bernoulli* distribution\n",
    "        - Each feature is binary (present/absent)\n",
    "        - <u>Frequency of words</u> is not considered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***\\*Take Home Concepts 1***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ - <u>Na&#239;ve Bayes</u> is a probabilistic model   \n",
    "$\\qquad$ - <u>Na&#239;ve Bayes</u> assumes all features are independent of each other given the class label  \n",
    "$\\qquad$ - For text classification problems, <u>Na&#239;ve Bayes</u> provides very strong baselines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Support Vector Machines**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Case study: *Sentiment analysis***\n",
    "*<p align=\"center\">![review](assets/sentiment_analysis_review.png)</p>*\n",
    "*<p align=\"center\" style=\"color:grey\">A movie review</p>*\n",
    "    - **Typical *sentimental indicators* in reviews**:\n",
    "        - *Positive*: wow, great, Bravo!\n",
    "        - *Negative*: boring, lame, worst\n",
    "\n",
    "- **Classifier**\n",
    "    - **Definition**: a function of input data $f(X)$\n",
    "    - **Examples**\n",
    "        - $f(\"\\mathrm{A\\ paragraph\\ of\\ medical\\ description}\")\\rightarrow\\{'nephrology','neurology','podiatry'\\}$\n",
    "        - $g(\"\\mathrm{A\\ paragraph\\ of\\ movie\\ review}\")\\rightarrow\\{+1,-1\\}$\n",
    "            - **Note**: +1 represents positive reviews, -1 represents negative reviews\n",
    "\n",
    "- **Decision boundary**\n",
    "    - **Definition**: a *boundary* that separates two different *labels*\n",
    "    - **Characteristics**: the shape is totally dependent on the application; can be used to identify unlabeled sample based on its location\n",
    "    - **Examples**:  \n",
    "\n",
    "    ![Alt text](assets/SVM_DB2.png)![Alt text](assets/SVM_DB1.png)\n",
    "    *<p align=\"center\" style=\"color:grey\">examples of decision boundary</p>*\n",
    "    - **Types**\n",
    "        - Irregular boundary\n",
    "            - Advantage: well defined for training data\n",
    "            - Disadvantage: lead to *overfitting* (bad generalization to test data) easily\n",
    "        - Linear boundary\n",
    "            - Advantage: easy to find and evaluate, better generalization (*Occam's razor*)\n",
    "            - Disadvantage: cannot be 100% correct even for training data\n",
    "        ![Alt text](assets/SVM_DB_Linearvsirregular.png)!\n",
    "        *<p align=\"center\" style=\"color:grey\">linear boundary vs. irregular boundary</p>*\n",
    "- **Support vector machine**\n",
    "    - **Definition**: a classifier that finds a hyperplane with *maximum margin* to separate two labels of data\n",
    "    - **Input**\n",
    "        - *Training data* with $X$: $\\textbf{x}_i = (x_1, x_2, ..., x_n), y \\in \\{-1,+1\\}$\n",
    "    - **Output**\n",
    "        - $f(X)$: $f(\\textbf{x}_i)=<w\\cdot \\textbf{x}_i>+b$\n",
    "            - $f(\\textbf{x}_i)\\ge 0\\rightarrow\\hat{y}_i = +1$\n",
    "            - $f(\\textbf{x}_i)\\le 0\\rightarrow\\hat{y}_i = -1$\n",
    "    - **Principle**: looks for *support vectors* (samples on the boundary; most sensitive to shift) and ensure they are not changed with small pertubations\n",
    "    ![svm](assets/SVM.png)\n",
    "    *<p align=\"center\" style=\"color:grey\">support vector machine</p>*\n",
    "    - **Advanced application: *classification for more than two labels***\n",
    "        - **Technique 1**: one vs. rest\n",
    "            - **Example**: nephrology vs. non-nephrology; neurology vs. non-neurology; podiatry vs. non-podiatry\n",
    "        ![svm-multiclass](assets/SVM_MC.png)\n",
    "        *<p align=\"center\" style=\"color:grey\">support vector machine: one vs. rest</p>*\n",
    "        - **Technique 2**: one vs. one\n",
    "            - **Example**: nephrology vs. neurology; nephrology vs. podiatry; neurology vs. podiatry\n",
    "        ![svm-multiclass](assets/SVM_MC2.png)\n",
    "        *<p align=\"center\" style=\"color:grey\">support vector machine: one vs. one</p>*\n",
    "    - **Hyperparameters**\n",
    "        - **Regulation parameter $C$**\n",
    "            - **Explanation**: importance of individual data points compared to better generalized model\n",
    "            - **Inference**: larger $C = $ less regularization\n",
    "                - $C \\uparrow \\Rightarrow $ fit training data as well as possible\n",
    "                - $C \\downarrow \\Rightarrow $ more tolerant to errors of individual data points to achieve simpler models\n",
    "        - **Kernel type**\n",
    "            - **Explanation**: algorithm for defining hyperplanes to separate labels\n",
    "            - **Availability**: linear (best for text data), rbf, polynomial\n",
    "        - **multi-class case or not**\n",
    "            - **Availability**: yes (ovr: one vs. rest, default; ovo: one vs.one), no\n",
    "        - **class weight**: if different features pose different importance, a different weight can be applied"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***\\*Take Home Concepts 2***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ - <u>SVM</u> tends to be the most accurate classifiers for text classification, especially in high-dimensional data  \n",
    "$\\qquad$ - <u>SVM</u> has a strong theoretical foundation  \n",
    "$\\qquad$ - <u>SVM</u> handles only numeric features  \n",
    "$\\qquad\\qquad$ - convert categorical features to numeric features  \n",
    "$\\qquad\\qquad$ - normalization required for large differences in numbers  \n",
    "$\\qquad$ - <u>SVM</u> results are hard to interpret"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
